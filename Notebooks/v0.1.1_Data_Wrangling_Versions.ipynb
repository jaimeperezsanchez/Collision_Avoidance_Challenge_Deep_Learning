{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Dataset Wrangling Versions.ipynb","provenance":[],"collapsed_sections":["WV_ujaY7IehD","uSSusyLlIehQ","twB8Cnw8Iehb","VI6txnuaIehl","PTubYN45Iehp","z_UMxIatIeht","2qJaOOvQIehw"]}},"cells":[{"cell_type":"code","metadata":{"id":"InwCKijYIeg8","colab_type":"code","outputId":"b7a1bd6a-4520-4f0f-ae0a-33a1cc3c7e49","executionInfo":{"status":"error","timestamp":1575283337706,"user_tz":-60,"elapsed":3486,"user":{"displayName":"Sergio PÃ©rez","photoUrl":"","userId":"01395943624135084959"}},"colab":{"base_uri":"https://localhost:8080/","height":355}},"source":["import pandas as pd\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', 100)\n","import numpy as np\n","import missingno\n","\n","from IPython.display import display\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","import plotly.express as px\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from category_encoders import BinaryEncoder"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fb18e2ef5090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"DmQFj3coIehC","colab_type":"text"},"source":["# 1. Wrangling for models\n","Docs: https://docs.google.com/document/d/1w7tmB_lGJ1YjbO7CiadMMcQ-diDAIrWJmFNgE7QW6IY/edit"]},{"cell_type":"markdown","metadata":{"id":"WV_ujaY7IehD","colab_type":"text"},"source":["## 1.1 Model v1\n","X -> time series from tca_max (tca=7) to tca=2 for each event. \n","(3d array: events, timestep, features)\n","\n","y -> minimum value from tca=2 to tca_min (tca~0) for each event.\n","(1d array)"]},{"cell_type":"markdown","metadata":{"id":"5gfWvkiuIehE","colab_type":"text"},"source":["### Input"]},{"cell_type":"code","metadata":{"id":"kLueTg-qIehF","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"train_data.csv\")\n","timestep = 17 #from 1 to 23 (17 with the current NaN strategy)\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bopHgc3IehI","colab_type":"text"},"source":["### Data Processing"]},{"cell_type":"code","metadata":{"id":"6ReozduuIehJ","colab_type":"code","outputId":"6c18e800-521a-44e1-ff7d-e001ae45ba0e","colab":{}},"source":["#Dropping first the empty column and then rows with NaNs\n","df = df.drop(\"c_rcs_estimate\", axis=1)\n","df = df.dropna(how='any')\n","\n","#Filtering events with len=1 or min_tca > 2 or max_tca < 2\n","def conditions(event):\n","    x = event[\"time_to_tca\"].values\n","    return ((x.min()<2.0) & (x.max()>2.0) & (x.shape[0]>1))\n","\n","df = df.groupby('event_id').filter(conditions)\n","\n","#OHE for c_object_type (5 categories) -> 5 new features\n","df = pd.get_dummies(df)\n","\n","#Binary encoder for mission_id (19 categories) -> 5 new features\n","encoder = BinaryEncoder(cols=['mission_id'], drop_invariant=True)\n","df = encoder.fit_transform(df)\n","\n","#Getting y as 1D-array\n","y = df.groupby([\"event_id\"])[\"risk\"].apply(lambda x: x.iloc[-1]).values.reshape(-1, 1)\n","\n","#Scaling y\n","_ = y_scaler.fit(df[\"risk\"].values.reshape(-1, 1)) #using the whole risk feature to scale the target 'y'\n","y = y_scaler.transform(y)\n","\n","#Getting X as df (dropping rows with tca < 2) \n","df = df.loc[df[\"time_to_tca\"]>2]\n","\n","#Adding feature 'event_length' for counting how many instances each event has\n","df[\"event_length\"] = df.groupby('event_id')['event_id'].transform('value_counts')\n","\n","#Scaling X\n","df = pd.DataFrame(X_scaler.fit_transform(df), columns=df.columns)\n","\n","#Transforming X into a 3D-array\n","events = df[\"event_id\"].nunique() #rows\n","features = len(df.columns) #columns\n","\n","X = np.zeros((events,timestep,features))\n","i = 0\n","\n","def df_to_3darray(event):\n","    global X, i\n","    #Transforming an event to time series (1,timesteps, columns)\n","    row = event.values.reshape(1,event.shape[0],event.shape[1])\n","    #Condition is needed to slice arrays correctly\n","    #Condition -> is timestep greater than the event's time series length? \n","    if(timestep>=row.shape[1]):\n","        X[i:i+1,-row.shape[1]:,:] = row\n","    else:\n","        X[i:i+1,:,:] = row[:,-timestep:,:]\n","    #index to iterate over X array\n","    i = i + 1\n","    #dataframe remains intact, while X array has been filled.\n","    return event\n","\n","df.groupby(\"event_id\").apply(df_to_3darray)\n","\n","#Dropping event_id to remove noise\n","X = X[:,:,1:]\n","\n","#TODO: Padding with specific values column-wise instead of zeros.\n","#TODO: Separating time dependent and independent feature in 2 X arrays\n","\n","print(X.shape, y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(7311, 17, 106) (7311, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B-Bz1ziIIehM","colab_type":"text"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"HmRubG33IehN","colab_type":"code","colab":{}},"source":["X, y, X_scaler, y_scaler, df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2A8V78sLIehQ","colab_type":"text"},"source":["# 2. Wrangling for feature selection"]},{"cell_type":"markdown","metadata":{"id":"uSSusyLlIehQ","colab_type":"text"},"source":["## 2.1 Feature Selection v1\n","Description: Emulating Model v1 w/o time series (3D-array)\n","\n","X -> Dataframe with data from tca_max (tca=7) to tca=2. \n","(2d df: instances, features)\n","\n","y -> minimum value from tca=2 to tca_min (tca~0) for each event repeated for all instances of each event.\n","(1d array)"]},{"cell_type":"markdown","metadata":{"id":"KMLmQIB9IehR","colab_type":"text"},"source":["### Input"]},{"cell_type":"code","metadata":{"id":"N4myLnPDIehS","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"train_data.csv\")\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g7wQffqFIehU","colab_type":"text"},"source":["### Data Processing"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pXcfsbzfIehV","colab_type":"code","outputId":"0d24c04f-f148-452f-9609-7f2dd6cfdda3","colab":{}},"source":["#Dropping first the empty column and then rows with NaNs\n","df = df.drop(\"c_rcs_estimate\", axis=1)\n","df = df.dropna(how='any')\n","\n","#Filtering events with len=1 or min_tca > 2 or max_tca < 2\n","def conditions(event):\n","    x = event[\"time_to_tca\"].values\n","    return ((x.min()<2.0) & (x.max()>2.0) & (x.shape[0]>1))\n","\n","df = df.groupby('event_id').filter(conditions)\n","\n","#OHE for c_object_type (5 categories) -> 5 new features\n","df = pd.get_dummies(df)\n","\n","#Binary encoder for mission_id (19 categories) -> 5 new features\n","encoder = BinaryEncoder(cols=['mission_id'], drop_invariant=True)\n","df = encoder.fit_transform(df)\n","\n","#Creating target feature as last risk value of an event\n","df[\"target-risk\"] = df.groupby('event_id')['risk'].transform(lambda x: x.iloc[-1]).values.reshape(-1, 1)\n","\n","#Slicing data (dropping rows with tca < 2) \n","df = df.loc[df[\"time_to_tca\"]>2]\n","\n","#Adding feature 'event_length' for counting how many instances each event has\n","df[\"event_length\"] = df.groupby('event_id')['event_id'].transform('value_counts')\n","\n","#Getting and scaling y\n","_ = y_scaler.fit(df[\"risk\"].values.reshape(-1, 1)) #using the whole risk feature to scale the target 'y'\n","y = y_scaler.transform(df[\"target-risk\"].values.reshape(-1, 1))\n","\n","#Getting and scaling X\n","X = df.drop([\"event_id\",\"target-risk\"], axis=1)\n","X = pd.DataFrame(X_scaler.fit_transform(X), columns=X.columns)\n","\n","print(X.shape, y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(85733, 106) (85733, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"awiHnpUJIehY","colab_type":"text"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"rkGKsm4qIehZ","colab_type":"code","colab":{}},"source":["X, y, X_scaler, y_scaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twB8Cnw8Iehb","colab_type":"text"},"source":["## 2.2 Feature Selection v2\n","Description: Minimum processing for predicting risk instance-wise instead of event-wise.\n","\n","X -> Raw dataframe processed. (2D - df)\n","\n","y -> Target feature (1d - array)"]},{"cell_type":"markdown","metadata":{"id":"4FSe29_SIehc","colab_type":"text"},"source":["### Input"]},{"cell_type":"code","metadata":{"id":"YZtQT9GHIehc","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"train_data.csv\")\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RnYwL02jIehf","colab_type":"text"},"source":["### Data Processing"]},{"cell_type":"code","metadata":{"id":"EicvQHrlIehg","colab_type":"code","outputId":"6dfc16fe-ba91-43be-ab0e-a9be3892f909","colab":{}},"source":["#Dropping first the empty column and then rows with NaNs\n","df = df.drop(\"c_rcs_estimate\", axis=1)\n","df = df.dropna(how='any')\n","\n","#OHE for c_object_type (5 categories) -> 5 new features\n","df = pd.get_dummies(df)\n","\n","#Binary encoder for mission_id (19 categories) -> 5 new features\n","encoder = BinaryEncoder(cols=['mission_id'], drop_invariant=True)\n","df = encoder.fit_transform(df)\n","\n","#Adding feature 'event_length' for counting how many instances each event has\n","df[\"event_length\"] = df.groupby('event_id')['event_id'].transform('value_counts')\n","\n","#Getting and scaling y\n","y = df[\"risk\"].values.reshape(-1, 1)\n","y = y_scaler.fit_transform(y)\n","\n","#Getting and Scaling X\n","X = df.drop([\"event_id\",\"risk\"], axis=1)\n","X = pd.DataFrame(X_scaler.fit_transform(X), columns=X.columns)\n","\n","print(X.shape, y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(143294, 105) (143294, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n0xdRNTiIehi","colab_type":"text"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"r7IfUDAMIehj","colab_type":"code","colab":{}},"source":["X, y, X_scaler, y_scaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VI6txnuaIehl","colab_type":"text"},"source":["## 2.3 Feature Selection v3.1\n","THIS VERSION DOES NOT SUPPORT FILLING\n","\n","Description: Time series analysis for each feature. Also valid for FFN\n","\n","X ->  dataframe with shifted columns and rows sliced as model version 1\n","\n","y ->  same as model version 1"]},{"cell_type":"markdown","metadata":{"id":"DHVDpKsjIehm","colab_type":"text"},"source":["### Input"]},{"cell_type":"code","metadata":{"id":"H_UA2EjkIehn","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"train_data.csv\")\n","features = [\"risk\"] #Array!! Time features that you want to analyze\n","timestep = 17 #from 1 to 23 (17 with the current NaN strategy)\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTubYN45Iehp","colab_type":"text"},"source":["### Data Processing"]},{"cell_type":"code","metadata":{"id":"_L9AKEZ_Iehq","colab_type":"code","outputId":"f71b2054-44ff-4f87-8500-7c5e426105c5","colab":{}},"source":["#Dropping first the empty column and then rows with NaNs\n","df = df.drop(\"c_rcs_estimate\", axis=1)\n","df = df.dropna(how='any')\n","\n","#Filtering events with len=1 or min_tca > 2 or max_tca < 2\n","def conditions(event):\n","    x = event[\"time_to_tca\"].values\n","    return ((x.min()<2.0) & (x.max()>2.0) & (x.shape[0]>1))\n","\n","df = df.groupby('event_id').filter(conditions)\n","\n","#OHE for c_object_type (5 categories) -> 5 new features\n","df = pd.get_dummies(df)\n","\n","#Binary encoder for mission_id (19 categories) -> 5 new features\n","encoder = BinaryEncoder(cols=['mission_id'], drop_invariant=True)\n","df = encoder.fit_transform(df)\n","\n","#Getting y as 1D-array\n","y = df.groupby([\"event_id\"])[\"risk\"].apply(lambda x: x.iloc[-1]).values.reshape(-1, 1)\n","\n","#Scaling y\n","_ = y_scaler.fit(df[\"risk\"].values.reshape(-1, 1)) #using the whole risk feature to scale the target 'y'\n","y = y_scaler.transform(y)\n","\n","#Getting X as df (dropping rows with tca < 2) \n","df = df.loc[df[\"time_to_tca\"]>2]\n","\n","#Adding feature 'event_length' for counting how many instances each event has\n","df[\"event_length\"] = df.groupby('event_id')['event_id'].transform('value_counts')\n","\n","#Scaling X\n","df = pd.DataFrame(X_scaler.fit_transform(df), columns=df.columns)\n","\n","#Shifting features\n","features.append('event_id')\n","X = pd.DataFrame()\n","\n","for feature in features:\n","    for i in range(timestep-1,-1,-1):\n","        X[feature+'_t-'+str(i)] = df[feature].shift(i)\n","    \n","#Getting last row -> getting one row per event\n","X = X.groupby([\"event_id_t-0\"]).apply(lambda x: x.iloc[-1])\n","X = X.reset_index(drop=True)\n","\n","#Deleting rows with more than one event_id\n","for i in range(timestep):\n","    X = X.loc[X[\"event_id_t-0\"]==X[\"event_id_t-\"+str(i)]]\n","\n","#Dropping y values not included in X due to last condition\n","y = np.take(y, X.index)\n","y = y.reshape(-1, 1)\n","\n","#Deleting event_id features\n","event_features = [feature for feature in list(X.columns) if feature.startswith('event_id_t-')]\n","X = X.drop(event_features, axis=1)\n","\n","print(X.shape, y.shape)\n","\n","#TODO: not dropping y values bc of X but filling X with zeros"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1, 85) (1, 1)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>relative_position_n_t-16</th>\n","      <th>relative_position_n_t-15</th>\n","      <th>relative_position_n_t-14</th>\n","      <th>relative_position_n_t-13</th>\n","      <th>relative_position_n_t-12</th>\n","      <th>relative_position_n_t-11</th>\n","      <th>relative_position_n_t-10</th>\n","      <th>relative_position_n_t-9</th>\n","      <th>relative_position_n_t-8</th>\n","      <th>relative_position_n_t-7</th>\n","      <th>relative_position_n_t-6</th>\n","      <th>relative_position_n_t-5</th>\n","      <th>relative_position_n_t-4</th>\n","      <th>relative_position_n_t-3</th>\n","      <th>relative_position_n_t-2</th>\n","      <th>relative_position_n_t-1</th>\n","      <th>relative_position_n_t-0</th>\n","      <th>relative_velocity_r_t-16</th>\n","      <th>relative_velocity_r_t-15</th>\n","      <th>relative_velocity_r_t-14</th>\n","      <th>relative_velocity_r_t-13</th>\n","      <th>relative_velocity_r_t-12</th>\n","      <th>relative_velocity_r_t-11</th>\n","      <th>relative_velocity_r_t-10</th>\n","      <th>relative_velocity_r_t-9</th>\n","      <th>relative_velocity_r_t-8</th>\n","      <th>relative_velocity_r_t-7</th>\n","      <th>relative_velocity_r_t-6</th>\n","      <th>relative_velocity_r_t-5</th>\n","      <th>relative_velocity_r_t-4</th>\n","      <th>relative_velocity_r_t-3</th>\n","      <th>relative_velocity_r_t-2</th>\n","      <th>relative_velocity_r_t-1</th>\n","      <th>relative_velocity_r_t-0</th>\n","      <th>relative_velocity_t_t-16</th>\n","      <th>relative_velocity_t_t-15</th>\n","      <th>relative_velocity_t_t-14</th>\n","      <th>relative_velocity_t_t-13</th>\n","      <th>relative_velocity_t_t-12</th>\n","      <th>relative_velocity_t_t-11</th>\n","      <th>relative_velocity_t_t-10</th>\n","      <th>relative_velocity_t_t-9</th>\n","      <th>relative_velocity_t_t-8</th>\n","      <th>relative_velocity_t_t-7</th>\n","      <th>relative_velocity_t_t-6</th>\n","      <th>relative_velocity_t_t-5</th>\n","      <th>relative_velocity_t_t-4</th>\n","      <th>relative_velocity_t_t-3</th>\n","      <th>relative_velocity_t_t-2</th>\n","      <th>relative_velocity_t_t-1</th>\n","      <th>relative_velocity_t_t-0</th>\n","      <th>relative_velocity_n_t-16</th>\n","      <th>relative_velocity_n_t-15</th>\n","      <th>relative_velocity_n_t-14</th>\n","      <th>relative_velocity_n_t-13</th>\n","      <th>relative_velocity_n_t-12</th>\n","      <th>relative_velocity_n_t-11</th>\n","      <th>relative_velocity_n_t-10</th>\n","      <th>relative_velocity_n_t-9</th>\n","      <th>relative_velocity_n_t-8</th>\n","      <th>relative_velocity_n_t-7</th>\n","      <th>relative_velocity_n_t-6</th>\n","      <th>relative_velocity_n_t-5</th>\n","      <th>relative_velocity_n_t-4</th>\n","      <th>relative_velocity_n_t-3</th>\n","      <th>relative_velocity_n_t-2</th>\n","      <th>relative_velocity_n_t-1</th>\n","      <th>relative_velocity_n_t-0</th>\n","      <th>t_time_lastob_start_t-16</th>\n","      <th>t_time_lastob_start_t-15</th>\n","      <th>t_time_lastob_start_t-14</th>\n","      <th>t_time_lastob_start_t-13</th>\n","      <th>t_time_lastob_start_t-12</th>\n","      <th>t_time_lastob_start_t-11</th>\n","      <th>t_time_lastob_start_t-10</th>\n","      <th>t_time_lastob_start_t-9</th>\n","      <th>t_time_lastob_start_t-8</th>\n","      <th>t_time_lastob_start_t-7</th>\n","      <th>t_time_lastob_start_t-6</th>\n","      <th>t_time_lastob_start_t-5</th>\n","      <th>t_time_lastob_start_t-4</th>\n","      <th>t_time_lastob_start_t-3</th>\n","      <th>t_time_lastob_start_t-2</th>\n","      <th>t_time_lastob_start_t-1</th>\n","      <th>t_time_lastob_start_t-0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>3927</td>\n","      <td>0.488575</td>\n","      <td>0.495217</td>\n","      <td>0.493934</td>\n","      <td>0.493933</td>\n","      <td>0.494542</td>\n","      <td>0.503581</td>\n","      <td>0.499421</td>\n","      <td>0.49669</td>\n","      <td>0.497545</td>\n","      <td>0.496896</td>\n","      <td>0.497114</td>\n","      <td>0.498001</td>\n","      <td>0.495155</td>\n","      <td>0.494288</td>\n","      <td>0.487686</td>\n","      <td>0.486254</td>\n","      <td>0.485287</td>\n","      <td>0.473191</td>\n","      <td>0.472494</td>\n","      <td>0.472643</td>\n","      <td>0.472643</td>\n","      <td>0.472568</td>\n","      <td>0.471598</td>\n","      <td>0.472046</td>\n","      <td>0.472345</td>\n","      <td>0.472245</td>\n","      <td>0.47232</td>\n","      <td>0.472295</td>\n","      <td>0.472195</td>\n","      <td>0.472494</td>\n","      <td>0.472593</td>\n","      <td>0.47329</td>\n","      <td>0.473439</td>\n","      <td>0.473564</td>\n","      <td>0.92693</td>\n","      <td>0.92693</td>\n","      <td>0.926924</td>\n","      <td>0.926924</td>\n","      <td>0.926924</td>\n","      <td>0.92693</td>\n","      <td>0.92693</td>\n","      <td>0.92693</td>\n","      <td>0.926924</td>\n","      <td>0.92693</td>\n","      <td>0.92693</td>\n","      <td>0.92693</td>\n","      <td>0.926924</td>\n","      <td>0.926924</td>\n","      <td>0.926924</td>\n","      <td>0.926924</td>\n","      <td>0.926924</td>\n","      <td>0.682228</td>\n","      <td>0.682228</td>\n","      <td>0.682228</td>\n","      <td>0.682228</td>\n","      <td>0.682228</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.682228</td>\n","      <td>0.682228</td>\n","      <td>0.682228</td>\n","      <td>0.682233</td>\n","      <td>0.682233</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      relative_position_n_t-16  relative_position_n_t-15  \\\n","3927                  0.488575                  0.495217   \n","\n","      relative_position_n_t-14  relative_position_n_t-13  \\\n","3927                  0.493934                  0.493933   \n","\n","      relative_position_n_t-12  relative_position_n_t-11  \\\n","3927                  0.494542                  0.503581   \n","\n","      relative_position_n_t-10  relative_position_n_t-9  \\\n","3927                  0.499421                  0.49669   \n","\n","      relative_position_n_t-8  relative_position_n_t-7  \\\n","3927                 0.497545                 0.496896   \n","\n","      relative_position_n_t-6  relative_position_n_t-5  \\\n","3927                 0.497114                 0.498001   \n","\n","      relative_position_n_t-4  relative_position_n_t-3  \\\n","3927                 0.495155                 0.494288   \n","\n","      relative_position_n_t-2  relative_position_n_t-1  \\\n","3927                 0.487686                 0.486254   \n","\n","      relative_position_n_t-0  relative_velocity_r_t-16  \\\n","3927                 0.485287                  0.473191   \n","\n","      relative_velocity_r_t-15  relative_velocity_r_t-14  \\\n","3927                  0.472494                  0.472643   \n","\n","      relative_velocity_r_t-13  relative_velocity_r_t-12  \\\n","3927                  0.472643                  0.472568   \n","\n","      relative_velocity_r_t-11  relative_velocity_r_t-10  \\\n","3927                  0.471598                  0.472046   \n","\n","      relative_velocity_r_t-9  relative_velocity_r_t-8  \\\n","3927                 0.472345                 0.472245   \n","\n","      relative_velocity_r_t-7  relative_velocity_r_t-6  \\\n","3927                  0.47232                 0.472295   \n","\n","      relative_velocity_r_t-5  relative_velocity_r_t-4  \\\n","3927                 0.472195                 0.472494   \n","\n","      relative_velocity_r_t-3  relative_velocity_r_t-2  \\\n","3927                 0.472593                  0.47329   \n","\n","      relative_velocity_r_t-1  relative_velocity_r_t-0  \\\n","3927                 0.473439                 0.473564   \n","\n","      relative_velocity_t_t-16  relative_velocity_t_t-15  \\\n","3927                   0.92693                   0.92693   \n","\n","      relative_velocity_t_t-14  relative_velocity_t_t-13  \\\n","3927                  0.926924                  0.926924   \n","\n","      relative_velocity_t_t-12  relative_velocity_t_t-11  \\\n","3927                  0.926924                   0.92693   \n","\n","      relative_velocity_t_t-10  relative_velocity_t_t-9  \\\n","3927                   0.92693                  0.92693   \n","\n","      relative_velocity_t_t-8  relative_velocity_t_t-7  \\\n","3927                 0.926924                  0.92693   \n","\n","      relative_velocity_t_t-6  relative_velocity_t_t-5  \\\n","3927                  0.92693                  0.92693   \n","\n","      relative_velocity_t_t-4  relative_velocity_t_t-3  \\\n","3927                 0.926924                 0.926924   \n","\n","      relative_velocity_t_t-2  relative_velocity_t_t-1  \\\n","3927                 0.926924                 0.926924   \n","\n","      relative_velocity_t_t-0  relative_velocity_n_t-16  \\\n","3927                 0.926924                  0.682228   \n","\n","      relative_velocity_n_t-15  relative_velocity_n_t-14  \\\n","3927                  0.682228                  0.682228   \n","\n","      relative_velocity_n_t-13  relative_velocity_n_t-12  \\\n","3927                  0.682228                  0.682228   \n","\n","      relative_velocity_n_t-11  relative_velocity_n_t-10  \\\n","3927                  0.682233                  0.682233   \n","\n","      relative_velocity_n_t-9  relative_velocity_n_t-8  \\\n","3927                 0.682233                 0.682233   \n","\n","      relative_velocity_n_t-7  relative_velocity_n_t-6  \\\n","3927                 0.682233                 0.682233   \n","\n","      relative_velocity_n_t-5  relative_velocity_n_t-4  \\\n","3927                 0.682233                 0.682228   \n","\n","      relative_velocity_n_t-3  relative_velocity_n_t-2  \\\n","3927                 0.682228                 0.682228   \n","\n","      relative_velocity_n_t-1  relative_velocity_n_t-0  \\\n","3927                 0.682233                 0.682233   \n","\n","      t_time_lastob_start_t-16  t_time_lastob_start_t-15  \\\n","3927                       0.0                       0.0   \n","\n","      t_time_lastob_start_t-14  t_time_lastob_start_t-13  \\\n","3927                       0.0                       0.0   \n","\n","      t_time_lastob_start_t-12  t_time_lastob_start_t-11  \\\n","3927                       0.0                       0.0   \n","\n","      t_time_lastob_start_t-10  t_time_lastob_start_t-9  \\\n","3927                       0.0                      0.0   \n","\n","      t_time_lastob_start_t-8  t_time_lastob_start_t-7  \\\n","3927                      0.0                      0.0   \n","\n","      t_time_lastob_start_t-6  t_time_lastob_start_t-5  \\\n","3927                      0.0                      0.0   \n","\n","      t_time_lastob_start_t-4  t_time_lastob_start_t-3  \\\n","3927                      0.0                      0.0   \n","\n","      t_time_lastob_start_t-2  t_time_lastob_start_t-1  \\\n","3927                      0.0                      0.0   \n","\n","      t_time_lastob_start_t-0  \n","3927                      0.0  "]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"z_UMxIatIeht","colab_type":"text"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"DqxDQ4mWIeht","colab_type":"code","colab":{}},"source":["X, y, X_scaler, y_scaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qJaOOvQIehw","colab_type":"text"},"source":["## 2.4 Feature Selection v3.2\n","THIS VERSION SUPPORTS FILLING\n","\n","Description: Time series analysis for each feature. Also valid for FFN.\n","\n","X ->  dataframe with shifted columns and rows sliced as model version 1\n","\n","y ->  same as model version 1"]},{"cell_type":"markdown","metadata":{"id":"3wFpbfLdIehx","colab_type":"text"},"source":["### Input"]},{"cell_type":"code","metadata":{"id":"nmd8dYm3Iehx","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"train_data.csv\")\n","features_ = [\"risk\"] #Array!! Time features that you want to analyze\n","timestep = 10 #from 1 to 23 (17 with the current NaN strategy)\n","X_scaler = MinMaxScaler()\n","y_scaler = MinMaxScaler()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bjC-fOXkIeh0","colab_type":"text"},"source":["### Data Processing"]},{"cell_type":"code","metadata":{"id":"N4nRUWMKIeh1","colab_type":"code","outputId":"7e83dc29-feb3-4cd9-8359-e10c2fdcb32b","colab":{}},"source":["#Dropping first the empty column and then rows with NaNs\n","df = df.drop(\"c_rcs_estimate\", axis=1)\n","df = df.dropna(how='any')\n","\n","#Filtering events with len=1 or min_tca > 2 or max_tca < 2\n","def conditions(event):\n","    x = event[\"time_to_tca\"].values\n","    return ((x.min()<2.0) & (x.max()>2.0) & (x.shape[0]>1))\n","\n","df = df.groupby('event_id').filter(conditions)\n","\n","#OHE for c_object_type (5 categories) -> 5 new features\n","df = pd.get_dummies(df)\n","\n","#Binary encoder for mission_id (19 categories) -> 5 new features\n","encoder = BinaryEncoder(cols=['mission_id'], drop_invariant=True)\n","df = encoder.fit_transform(df)\n","\n","#Getting y as 1D-array\n","y = df.groupby([\"event_id\"])[\"risk\"].apply(lambda x: x.iloc[-1]).values.reshape(-1, 1)\n","\n","#Scaling y\n","_ = y_scaler.fit(df[\"risk\"].values.reshape(-1, 1)) #using the whole risk feature to scale the target 'y'\n","y = y_scaler.transform(y)\n","\n","#Getting X as df (dropping rows with tca < 2) \n","df = df.loc[df[\"time_to_tca\"]>2]\n","\n","#Adding feature 'event_length' for counting how many instances each event has\n","df[\"event_length\"] = df.groupby('event_id')['event_id'].transform('value_counts')\n","\n","#Scaling X\n","df = pd.DataFrame(X_scaler.fit_transform(df), columns=df.columns)\n","\n","#Selecting features\n","features_.insert(0, 'event_id')\n","df = df[features_]\n","\n","#Transforming X into a 3D-array\n","events = df[\"event_id\"].nunique() #rows\n","features = len(df.columns) #columns\n","event_id\n","X = np.zeros((events,timestep,features))\n","i = 0\n","\n","def df_to_3darray(event):\n","    global X, i\n","    #Transforming an event to time series (1,timesteps, columns)\n","    row = event.values.reshape(1,event.shape[0],event.shape[1])\n","    #Condition is needed to slice arrays correctly\n","    #Condition -> is timestep greater than the event's time series length? \n","    if(timestep>=row.shape[1]):\n","        X[i:i+1,-row.shape[1]:,:] = row\n","    else:\n","        X[i:i+1,:,:] = row[:,-timestep:,:]\n","    #index to iterate over X array\n","    i = i + 1\n","    #dataframe remains intact, while X array has been filled.\n","    return event\n","\n","df.groupby(\"event_id\").apply(df_to_3darray)\n","\n","#Dropping event_id to remove noise\n","X = X[:,:,1:]\n","\n","#Reshaping again to 2D array but now events are filled\n","X = X.reshape(X.shape[0], timestep*X.shape[2])\n","\n","#Naming shifted columns\n","shifted_columns = []\n","original_columns = list(df.columns)[1:] #Dropping event_id\n","\n","for i in range(timestep-1,-1,-1):\n","    for column in original_columns: \n","        shifted_columns.append(column+\"_t-\"+str(i))\n","        \n","#Creating df from reshape array and shifted column names\n","X = pd.DataFrame(X, columns=shifted_columns)\n","\n","print(X.shape, y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(7311, 10) (7311, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rmDIy0jXIeh5","colab_type":"text"},"source":["### Output"]},{"cell_type":"code","metadata":{"id":"Y0oyBZP_Ieh7","colab_type":"code","colab":{}},"source":["X, y, scaler_X, scaler_y"],"execution_count":0,"outputs":[]}]}